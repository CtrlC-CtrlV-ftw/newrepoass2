{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow RELU Network without Bias (to predict XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.uniform(-1, 1, size=(input_size, hidden_size))\n",
    "        self.W2 = np.random.uniform(-1, 1, size=(hidden_size, output_size))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1)\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2 )\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Gradient of loss w.r.t. Z2\n",
    "        # dLoss_dZ2 = np.dot(-2, (y - self.z2)/y) relative doesn't work atm\n",
    "        dLoss_dZ2 = np.dot(-2, y - self.z2)\n",
    "        \n",
    "        # Gradient of loss w.r.t. W2\n",
    "        dLoss_dW2 = np.dot(dLoss_dZ2.T, self.a1)\n",
    "        \n",
    "        # Gradient of loss w.r.t. A1\n",
    "        dLoss_dA1 = np.dot(self.W2, dLoss_dZ2.T)\n",
    "        \n",
    "        # Gradient of loss w.r.t. Z1\n",
    "        dLoss_dZ1 = (self.a1 > 0).T * dLoss_dA1\n",
    "        \n",
    "        # Gradient of loss w.r.t. W1\n",
    "        dLoss_dW1 = np.dot(dLoss_dZ1, X)\n",
    "\n",
    "        self.W2 -= learning_rate * dLoss_dW2.T\n",
    "        self.W1 -= learning_rate * dLoss_dW1.T\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = np.mean((output - y) ** 2)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.10f}\")\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]]).reshape(-1, 1)\n",
    "print(f'y.shape: {y.shape}')\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "loss_history = network.train(X, y, num_epochs, learning_rate)\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow RELU Network with Bias (to predict XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases randomly\n",
    "        self.W1 = np.random.uniform(-1, 1, size=(input_size, hidden_size))\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.uniform(-1, 1, size=(hidden_size, output_size))\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "            # Gradient of loss w.r.t. Z2\n",
    "            dLoss_dZ2 = np.dot(-2, (y - self.z2))\n",
    "            \n",
    "            # Gradient of loss w.r.t. W2\n",
    "            dLoss_dW2 = np.dot(dLoss_dZ2.T, self.a1)\n",
    "\n",
    "            # Gradient of bias B2\n",
    "            dLoss_db2 = np.sum(dLoss_dZ2, axis=0)\n",
    "            \n",
    "            # Gradient of loss w.r.t. A1\n",
    "            dLoss_dA1 = np.dot(self.W2, dLoss_dZ2.T)\n",
    "            \n",
    "            # Gradient of loss w.r.t. Z1\n",
    "            dLoss_dZ1 = (self.a1 > 0) * dLoss_dA1.T\n",
    "            \n",
    "            # Gradient of loss w.r.t. W1\n",
    "            dLoss_dW1 = np.dot(dLoss_dZ1.T, X)\n",
    "\n",
    "            # Gradient of bias B1\n",
    "            dLoss_db1 = np.sum(dLoss_dZ1, axis=0, keepdims=True)\n",
    "\n",
    "            self.W2 -= learning_rate * dLoss_dW2.T\n",
    "            self.W1 -= learning_rate * dLoss_dW1.T\n",
    "            self.b2 -= learning_rate * dLoss_db2\n",
    "            self.b1 -= learning_rate * dLoss_db1\n",
    "\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = np.mean((output - y) ** 2)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.10f}\")\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "loss_history = network.train(X, y, num_epochs, learning_rate)\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with $c^2=S(gh, g \\sqrt{h\\lambda})$ (WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "g = 9.80665  # Gravitational acceleration (m/s^2)\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "h = data['height'].values\n",
    "lambda_w = data['wave_length'].values\n",
    "c = data['speed'].values\n",
    "\n",
    "# Prepare the features based on c^2 = S(gh, g sqrt(h lambda))\n",
    "X = np.vstack((g * h, g * np.sqrt(h * lambda_w))).T\n",
    "y = c**2  # Target is c^2\n",
    "\n",
    "# Normalize the features\n",
    "mean_X = np.mean(X, axis=0)\n",
    "std_X = np.std(X, axis=0)\n",
    "X_scaled = (X - mean_X) / std_X\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = 2\n",
    "hidden_size = 32\n",
    "output_size = 1\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "# Mean Squared Relative Error Loss\n",
    "def msre_loss(outputs, targets, epsilon=1e-8):\n",
    "    return np.mean(((targets - outputs) / (targets + epsilon)) ** 2)\n",
    "\n",
    "def clip_gradient(grad, max_norm):\n",
    "    norm = np.linalg.norm(grad)\n",
    "    if norm > max_norm:\n",
    "        grad = grad * (max_norm / norm)\n",
    "    return grad\n",
    "\n",
    "# Forward pass\n",
    "def forward(X):\n",
    "    Z1 = X.dot(W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1.dot(W2) + b2\n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Training loop\n",
    "epochs = 100000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1, A1, Z2 = forward(X_scaled)\n",
    "    loss = msre_loss(Z2, y.reshape(-1, 1))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    dZ2 = 2 * (Z2 - y.reshape(-1, 1)) / y.size\n",
    "    dW2 = A1.T.dot(dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0)\n",
    "    \n",
    "    dA1 = dZ2.dot(W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = X_scaled.T.dot(dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0)\n",
    "    \n",
    "    dW1 = clip_gradient(dW1, max_norm=1.0)\n",
    "    db1 = clip_gradient(db1, max_norm=1.0)\n",
    "    dW2 = clip_gradient(dW2, max_norm=1.0)\n",
    "    db2 = clip_gradient(db2, max_norm=1.0)\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    if (epoch + 1) % 5000 == 0:\n",
    "        rmsre = np.sqrt(loss)  # Calculate RMSRE for reporting\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss (MSRE): {loss:.6f}, RMSRE: {rmsre:.6f}')\n",
    "\n",
    "# Predictions and denormalization\n",
    "_, _, y_pred = forward(X_scaled)\n",
    "predicted_speeds = np.sqrt(np.maximum(y_pred.flatten(), 0))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(c, predicted_speeds, color='red')\n",
    "plt.plot([c.min(), c.max()], [c.min(), c.max()], 'k--', lw=4)\n",
    "plt.xlabel('Actual Speed (m/s)')\n",
    "plt.ylabel('Predicted Speed (m/s)')\n",
    "plt.title('Actual vs. Predicted Wave Speeds')\n",
    "plt.show()\n",
    "\n",
    "# Load the verification data\n",
    "verification_data = pd.read_csv('water_waves_verification_data_input.csv')\n",
    "h_ver = verification_data['h'].values\n",
    "lambda_w_ver = verification_data['lam'].values\n",
    "\n",
    "# Transform the verification data for the first approach\n",
    "X_ver_1 = np.vstack((g * h_ver, g * np.sqrt(h_ver * lambda_w_ver))).T\n",
    "\n",
    "# Normalize the features using the mean and std from the training data\n",
    "X_ver_scaled_1 = (X_ver_1 - mean_X) / std_X\n",
    "\n",
    "# Predict using the trained model for approach 1\n",
    "_, _, y_pred_ver_1 = forward(X_ver_scaled_1)\n",
    "expected_speed_1 = np.sqrt(np.maximum(y_pred_ver_1.flatten(), 0))\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission_1 = pd.DataFrame({\n",
    "    'id': verification_data['id'],\n",
    "    'expected': expected_speed_1\n",
    "})\n",
    "\n",
    "# Check if submission has null values (sometimes it does)\n",
    "print(submission_1.isnull().values.any())\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(submission_1)\n",
    "\n",
    "## create csv fild for submission\n",
    "#submission_1.to_csv('submission_approach_1v5.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "submission_1.to_csv('submission_approach_1v7.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Stochastic Gradient Descent (Not tested yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "g = 9.80665  # Gravitational acceleration (m/s^2)\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "h = data['height'].values\n",
    "lambda_w = data['wave_length'].values\n",
    "c = data['speed'].values\n",
    "\n",
    "# Prepare the features based on c^2 = S(gh, g sqrt(h lambda))\n",
    "X = np.vstack((g * h, g * np.sqrt(h * lambda_w))).T\n",
    "y = c**2  # Target is c^2\n",
    "\n",
    "# Normalize the features\n",
    "mean_X = np.mean(X, axis=0)\n",
    "std_X = np.std(X, axis=0)\n",
    "X_scaled = (X - mean_X) / std_X\n",
    "\n",
    "# Split the dataset into training and validation sets (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 2\n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "learning_rate = 1e-7  # Adjusted learning rate\n",
    "batch_size = 64        # Reasonable batch size\n",
    "epochs = 5000          # Reduced for quicker iteration\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "# Mean Squared Relative Error Loss\n",
    "def msre_loss(outputs, targets, epsilon=1e-8):\n",
    "    return np.mean(((targets - outputs) / (targets + epsilon)) ** 2)\n",
    "\n",
    "# Forward pass function\n",
    "def forward(X):\n",
    "    Z1 = X.dot(W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1.dot(W2) + b2\n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Compute gradients function\n",
    "def compute_gradients(X, y, A1, Z1, Z2):\n",
    "    m = X.shape[0]  # Number of examples\n",
    "    \n",
    "    # Backpropagation\n",
    "    dZ2 = 2 * (Z2 - y.reshape(-1, 1)) / m\n",
    "    dW2 = A1.T.dot(dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0)\n",
    "    \n",
    "    dA1 = dZ2.dot(W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = X.T.dot(dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Training loop with mini-batch SGD\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the training set\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Mini-batch\n",
    "        X_batch = X_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        Z1, A1, Z2 = forward(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = msre_loss(Z2, y_batch)\n",
    "        \n",
    "        # Backpropagation\n",
    "        dW1, db1, dW2, db2 = compute_gradients(X_batch, y_batch, A1, Z1, Z2)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "    \n",
    "    # Compute training loss\n",
    "    _, _, Z2_train = forward(X_train)\n",
    "    train_loss = msre_loss(Z2_train, y_train)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Compute validation loss\n",
    "    _, _, Z2_val = forward(X_val)\n",
    "    val_loss = msre_loss(Z2_val, y_val)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        train_rmsre = np.sqrt(train_loss)  # Calculate RMSRE for reporting\n",
    "        val_rmsre = np.sqrt(val_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], MSRE, RMSRE - Training: {train_loss:.6f}, {train_rmsre:.6f} - Validation: {val_loss:.6f}, {val_rmsre:.6f}')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSRE Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Predictions and denormalization on training data\n",
    "_, _, y_pred_train = forward(X_train)\n",
    "predicted_speeds_train = np.sqrt(np.maximum(y_pred_train.flatten(), 0))\n",
    "\n",
    "# Predictions and denormalization on validation data\n",
    "_, _, y_pred_val = forward(X_val)\n",
    "predicted_speeds_val = np.sqrt(np.maximum(y_pred_val.flatten(), 0))\n",
    "\n",
    "# Visualization of training results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(np.sqrt(y_train), predicted_speeds_train, color='blue', label='Training')\n",
    "plt.scatter(np.sqrt(y_val), predicted_speeds_val, color='red', label='Validation')\n",
    "plt.plot([c.min(), c.max()], [c.min(), c.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Speed (m/s)')\n",
    "plt.ylabel('Predicted Speed (m/s)')\n",
    "plt.title('Training vs Validation Predicted Speeds')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Transform the verification data for the first approach\n",
    "X_ver_1 = np.vstack((g * h_ver, g * np.sqrt(h_ver * lambda_w_ver))).T\n",
    "X_ver_scaled_1 = (X_ver_1 - mean_X) / std_X\n",
    "\n",
    "# Predict using the trained model for test data\n",
    "_, _, y_pred_ver_1 = forward(X_ver_scaled_1)\n",
    "expected_speed_1 = np.sqrt(np.maximum(y_pred_ver_1.flatten(), 0))\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission_1 = pd.DataFrame({\n",
    "    'id': verification_data['id'],\n",
    "    'expected': expected_speed_1\n",
    "})\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(submission_1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient descent with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "g = 9.80665  # Gravitational acceleration (m/s^2)\n",
    "\n",
    "class WaterWavePredictor:\n",
    "    def __init__(self, input_size=2, hidden_size=256, output_size=1, learning_rate=1e-1, max_norm=1.0):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_norm = max_norm\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2. / self.input_size)\n",
    "        self.b1 = np.zeros(self.hidden_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2. / self.hidden_size)\n",
    "        self.b2 = np.zeros(self.output_size)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(x.dtype)\n",
    "\n",
    "    def msre_loss(self, outputs, targets, epsilon=1e-8):\n",
    "        return np.mean(((targets - outputs) / (targets + epsilon)) ** 2)\n",
    "\n",
    "    def clip_gradient(self, grad):\n",
    "        norm = np.linalg.norm(grad)\n",
    "        if norm > self.max_norm:\n",
    "            grad = grad * (self.max_norm / norm)\n",
    "        return grad\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z1 = X.dot(self.W1) + self.b1\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.W2) + self.b2\n",
    "        return Z1, A1, Z2\n",
    "\n",
    "    def backward(self, X, Z1, A1, Z2, y):\n",
    "        dZ2 = 2 * (Z2 - y.reshape(-1, 1)) / y.size\n",
    "        dW2 = A1.T.dot(dZ2)\n",
    "        db2 = np.sum(dZ2, axis=0)\n",
    "\n",
    "        dA1 = dZ2.dot(self.W2.T)\n",
    "        dZ1 = dA1 * self.relu_derivative(Z1)\n",
    "        dW1 = X.T.dot(dZ1)\n",
    "        db1 = np.sum(dZ1, axis=0)\n",
    "\n",
    "        dW1 = self.clip_gradient(dW1)\n",
    "        db1 = self.clip_gradient(db1)\n",
    "        dW2 = self.clip_gradient(dW2)\n",
    "        db2 = self.clip_gradient(db2)\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100000):\n",
    "        losses_train = []\n",
    "        losses_val = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            Z1_train, A1_train, Z2_train = self.forward(X_train)\n",
    "            loss_train = self.msre_loss(Z2_train, y_train.reshape(-1, 1))\n",
    "            losses_train.append(loss_train)\n",
    "\n",
    "            _, _, Z2_val = self.forward(X_val)\n",
    "            loss_val = self.msre_loss(Z2_val, y_val.reshape(-1, 1))\n",
    "            losses_val.append(loss_val)\n",
    "\n",
    "            self.backward(X_train, Z1_train, A1_train, Z2_train, y_train)\n",
    "\n",
    "            if (epoch + 1) % 5000 == 0:\n",
    "                rmsre_train = np.sqrt(loss_train)\n",
    "                rmsre_val = np.sqrt(loss_val)\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Loss (MSRE) Train: {loss_train:.6f}, RMSRE Train: {rmsre_train:.6f}, Loss (MSRE) Val: {loss_val:.6f}, RMSRE Val: {rmsre_val:.6f}')\n",
    "\n",
    "        return losses_train, losses_val\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, Z2 = self.forward(X)\n",
    "        return np.sqrt(np.maximum(Z2.flatten(), 0))\n",
    "\n",
    "    def visualize_losses(self, losses_train, losses_val):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(losses_train, label='Training Loss (MSRE)')\n",
    "        plt.plot(losses_val, label='Validation Loss (MSRE)')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss (MSRE)')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss (MSRE)')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_predictions(self, y_actual, y_train_pred, y_val_pred):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_actual, y_train_pred, color='blue', label='Training Predictions')\n",
    "        plt.scatter(y_actual, y_val_pred, color='green', label='Validation Predictions')\n",
    "        plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'k--', lw=4)\n",
    "        plt.xlabel('Actual Speed (m/s)')\n",
    "        plt.ylabel('Predicted Speed (m/s)')\n",
    "        plt.legend()\n",
    "        plt.title('Actual vs Predicted Wave Speeds (Training and Validation)')\n",
    "        plt.show()\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "h = data['height'].values\n",
    "lambda_w = data['wave_length'].values\n",
    "c = data['speed'].values\n",
    "\n",
    "# Prepare the features based on c^2 = S(gh, g sqrt(h lambda))\n",
    "X = np.vstack((g * h, g * np.sqrt(h * lambda_w))).T\n",
    "y = c**2  # Target is c^2\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using the training set statistics\n",
    "mean_X = np.mean(X_train, axis=0)\n",
    "std_X = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - mean_X) / std_X\n",
    "X_val_scaled = (X_val - mean_X) / std_X\n",
    "\n",
    "# Initialize the WaterWavePredictor class\n",
    "predictor = WaterWavePredictor()\n",
    "\n",
    "# Train the model\n",
    "losses_train, losses_val = predictor.train(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "\n",
    "# Predictions and denormalization for training and validation sets\n",
    "predicted_speeds_train = predictor.predict(X_train_scaled)\n",
    "predicted_speeds_val = predictor.predict(X_val_scaled)\n",
    "\n",
    "# Visualization - Training vs Validation Loss\n",
    "predictor.visualize_losses(losses_train, losses_val)\n",
    "\n",
    "# Visualization - Actual vs Predicted for Training and Validation sets\n",
    "predictor.visualize_predictions(c, predicted_speeds_train, predicted_speeds_val)\n",
    "\n",
    "# Load the verification data\n",
    "verification_data = pd.read_csv('water_waves_verification_data_input.csv')\n",
    "h_ver = verification_data['h'].values\n",
    "lambda_w_ver = verification_data['lam'].values\n",
    "\n",
    "# Transform the verification data\n",
    "X_ver = np.vstack((g * h_ver, g * np.sqrt(h_ver * lambda_w_ver))).T\n",
    "\n",
    "# Normalize the features using the mean and std from the training data\n",
    "X_ver_scaled = (X_ver - mean_X) / std_X\n",
    "\n",
    "# Predict using the trained model\n",
    "expected_speed = predictor.predict(X_ver_scaled)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': verification_data['id'],\n",
    "    'expected': expected_speed\n",
    "})\n",
    "\n",
    "# Check if submission has null values\n",
    "print(submission.isnull().values.any())\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(submission)\n",
    "\n",
    "# Create CSV file for submission\n",
    "# submission.to_csv('submission_approach_with_class.csv', index=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
