{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    # Apply the ReLU activation function element-wise\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def heaviside(x):\n",
    "    # Apply the Heaviside step function element-wise\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def forward_pass(X, W1, W3):\n",
    "    # Compute the input to the hidden layer\n",
    "    X1 = np.dot(X, W1)\n",
    "    \n",
    "    # Apply ReLU activation to the hidden layer\n",
    "    X2 = relu(X1)\n",
    "    \n",
    "    # Compute the output of the network\n",
    "    X3 = np.dot(X2, W3)\n",
    "    \n",
    "    return X1, X2, X3\n",
    "\n",
    "def backward_pass(X, y, W1, W3, X1, X2, X3):\n",
    "    # Compute the error gradient at the output layer\n",
    "    dE_dX3 = -(y - X3) \n",
    "\n",
    "    # Compute the gradient of the loss with respect to the output layer weights\n",
    "    dE_dW3 = dE_dX3 @ X2\n",
    "\n",
    "    # Propagate the error gradient back to the hidden layer\n",
    "    dE_dX2 = W3 @ dE_dX3\n",
    "\n",
    "    # Compute the error gradient at the input of the hidden layer\n",
    "    dE_dX1 = heaviside(X2).T * dE_dX2\n",
    "    \n",
    "    # Compute the gradient of the loss with respect to the hidden layer weights\n",
    "    dE_dW1 = dE_dX1 @ X \n",
    "\n",
    "    return dE_dW1, dE_dW3\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0.5, 0.8]])  # Input data\n",
    "y = np.array([[1.0]])  # Ground truth labels\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Perform forward pass\n",
    "X1, X2, X3 = forward_pass(X, W1, W2)\n",
    "\n",
    "# Perform backward pass\n",
    "dE_dW1, dE_dW3 = backward_pass(X, y, W1, W2, X1, X2, X3)\n",
    "\n",
    "print(f\"Gradient with respect to W1: \\n {dE_dW1} \\n\")\n",
    "print(f\"Gradient with respect to W3: \\n {dE_dW3}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def relu(x):\n",
    "    # Apply the ReLU activation function element-wise\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def heaviside(x):\n",
    "    # Apply the Heaviside step function element-wise\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def forward_pass(X, W1, W2):\n",
    "    # Compute the input to the hidden layer\n",
    "    X1 = np.dot(X, W1)\n",
    "    \n",
    "    # Apply ReLU activation to the hidden layer\n",
    "    X2 = relu(X1)\n",
    "    \n",
    "    # Compute the output of the network\n",
    "    X3 = np.dot(X2, W2)\n",
    "    \n",
    "    return X1, X2, X3\n",
    "\n",
    "def backward_pass(X, y, W1, W2, X1, X2, X3, learning_rate):\n",
    "    # Compute the error gradient at the output layer\n",
    "    dE_dX3 = -(y - X3) \n",
    "\n",
    "    # Compute the gradient of the loss with respect to the output layer weights\n",
    "    dE_dW2 = np.dot(X2.T, dE_dX3)\n",
    "\n",
    "    # Propagate the error gradient back to the hidden layer\n",
    "    dE_dX2 = np.dot(dE_dX3, W2.T)\n",
    "\n",
    "    # Compute the error gradient at the input of the hidden layer\n",
    "    dE_dX1 = heaviside(X2) * dE_dX2\n",
    "    \n",
    "    # Compute the gradient of the loss with respect to the hidden layer weights\n",
    "    dE_dW1 = np.dot(X.T, dE_dX1)\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    W1 -= learning_rate * dE_dW1\n",
    "    W2 -= learning_rate * dE_dW2\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Extract the input features and target variable\n",
    "X = training_data[['height', 'wave_length']].values\n",
    "y = training_data['speed'].values.reshape(-1, 1)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(X)):\n",
    "        # Get a single training sample\n",
    "        X_sample = X[i].reshape(1, -1)\n",
    "        y_sample = y[i].reshape(1, -1)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        X1, X2, X3 = forward_pass(X_sample, W1, W2)\n",
    "        \n",
    "        # Perform backward pass and update weights\n",
    "        W1, W2 = backward_pass(X_sample, y_sample, W1, W2, X1, X2, X3, learning_rate)\n",
    "    \n",
    "    # Print the mean squared error every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        y_pred = forward_pass(X, W1, W2)[-1]\n",
    "        mse = np.mean((y - y_pred)**2)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Mean Squared Error: {mse:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Extract the input features and target variable\n",
    "X = training_data[['height', 'wave_length']].values\n",
    "y = training_data['speed'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Initialize weights randomly\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Get a single training sample\n",
    "        X_sample = X_train[i].reshape(1, -1)\n",
    "        y_sample = y_train[i].reshape(1, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        hidden_layer = np.maximum(0, np.dot(X_sample, W1))\n",
    "        output = np.dot(hidden_layer, W2)\n",
    "        \n",
    "        # Backward pass\n",
    "        output_delta = output - y_sample\n",
    "        hidden_error = np.dot(output_delta, W2.T)\n",
    "        hidden_delta = hidden_error * (hidden_layer > 0)\n",
    "        \n",
    "        # Update weights\n",
    "        W2 -= learning_rate * np.dot(hidden_layer.T, output_delta)\n",
    "        W1 -= learning_rate * np.dot(X_sample.T, hidden_delta)\n",
    "    \n",
    "    # Print the mean squared error every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        hidden_layer = np.maximum(0, np.dot(X_test, W1))\n",
    "        y_pred = np.dot(hidden_layer, W2)\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Mean Squared Error: {mse:.10f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Extract the input features and target variable\n",
    "X = training_data[['height', 'wave_length']].values\n",
    "y = training_data['speed'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Initialize weights randomly\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Get a single training sample\n",
    "        X_sample = X_train[i].reshape(1, -1)\n",
    "        y_sample = y_train[i].reshape(1, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        X1 = np.dot(X_sample, W1)\n",
    "        X2 = np.maximum(0, X1)\n",
    "        X3 = np.dot(X2, W2)\n",
    "        \n",
    "        # Backward pass\n",
    "        dE_dX3 = X3 - y_sample\n",
    "        dE_dW2 = np.dot(X2.T, dE_dX3)\n",
    "        dE_dX2 = np.dot(dE_dX3, W2.T)\n",
    "        dE_dX1 = dE_dX2 * (X1 > 0)\n",
    "        dE_dW1 = np.dot(X_sample.T, dE_dX1)\n",
    "        \n",
    "        # Update weights\n",
    "        W2 -= learning_rate * dE_dW2\n",
    "        W1 -= learning_rate * dE_dW1\n",
    "    \n",
    "    # Print the mean squared error every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        X1_test = np.dot(X_test, W1)\n",
    "        X2_test = np.maximum(0, X1_test)\n",
    "        y_pred = np.dot(X2_test, W2)\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Mean Squared Error: {mse:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# Extract the input features (MedInc and AveRooms) and target variable (MedHouseVal)\n",
    "X = california.data[:, [0, 5]]  # MedInc: Median income, AveRooms: Average number of rooms\n",
    "y = california.target.reshape(-1, 1)\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize weights randomly\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # Get a single training sample\n",
    "        X_sample = X_train[i].reshape(1, -1)\n",
    "        y_sample = y_train[i].reshape(1, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        X1 = np.dot(X_sample, W1)\n",
    "        X2 = np.maximum(0, X1)\n",
    "        X3 = np.dot(X2, W2)\n",
    "        \n",
    "        # Backward pass\n",
    "        dE_dX3 = X3 - y_sample\n",
    "        dE_dW2 = np.dot(X2.T, dE_dX3)\n",
    "        dE_dX2 = np.dot(dE_dX3, W2.T)\n",
    "        dE_dX1 = dE_dX2 * (X1 > 0)\n",
    "        dE_dW1 = np.dot(X_sample.T, dE_dX1)\n",
    "        \n",
    "        # Update weights\n",
    "        W2 -= learning_rate * dE_dW2\n",
    "        W1 -= learning_rate * dE_dW1\n",
    "    \n",
    "    # Print the mean squared error every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        X1_test = np.dot(X_test, W1)\n",
    "        X2_test = np.maximum(0, X1_test)\n",
    "        y_pred = np.dot(X2_test, W2)\n",
    "        mse = np.mean((y_test - y_pred)**2)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Mean Squared Error: {mse:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "g = 9.80665  # Gravitational acceleration\n",
    "\n",
    "# Compute dimensionally homogeneous and normalized inputs/output\n",
    "c4 = training_data['speed']**4\n",
    "training_data['g2h2_c4'] = (g**2 * training_data['height']**2) / c4\n",
    "training_data['g2hl_c4'] = (g**2 * training_data['height'] * training_data['wave_length']) / c4\n",
    "X = training_data[[\"g2h2_c4\", \"g2hl_c4\"]].values\n",
    "y = (c4 / c4).values.reshape(-1, 1)  # Normalized target (always 1)\n",
    "\n",
    "X = training_data[['height', 'wave_length']]\n",
    "y = training_data.speed.values.reshape(-1, 1) \n",
    "\n",
    "# Split data (80/20)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Network parameters\n",
    "input_dim, hidden_dim, output_dim = 2, 2, 1\n",
    "\n",
    "# Initialize weights\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Learning rate and epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        # Get a single training sample\n",
    "        X_sample = X_train[i].reshape(1, -1)\n",
    "        y_sample = y_train[i].reshape(1, -1)\n",
    "\n",
    "        # Forward pass\n",
    "        X1 = np.dot(X_sample, W1)\n",
    "        X2 = np.maximum(0, X1)  # ReLU activation\n",
    "        X3 = np.dot(X2, W2)\n",
    "\n",
    "        # Backward pass\n",
    "        dE_dX3 = X3 - y_sample\n",
    "        dE_dW2 = np.dot(X2.T, dE_dX3)\n",
    "        dE_dX2 = np.dot(dE_dX3, W2.T)\n",
    "        dE_dX1 = dE_dX2 * (X1 > 0)\n",
    "        dE_dW1 = np.dot(X_sample.T, dE_dX1)\n",
    "\n",
    "        # Update weights\n",
    "        W2 -= learning_rate * dE_dW2\n",
    "        W1 -= learning_rate * dE_dW1\n",
    "\n",
    "    # Calculate and print MSE for both training and testing sets\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Training MSE\n",
    "        X1_train = np.dot(X_train, W1)\n",
    "        X2_train = np.maximum(0, X1_train)\n",
    "        y_pred_train = np.dot(X2_train, W2)\n",
    "        mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "\n",
    "        # Testing MSE\n",
    "        X1_test = np.dot(X_test, W1)\n",
    "        X2_test = np.maximum(0, X1_test)\n",
    "        y_pred_test = np.dot(X2_test, W2)\n",
    "        mse_test = np.mean((y_test - y_pred_test)**2)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train MSE: {mse_train:.4f}, Test MSE: {mse_test:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def relu(x):\n",
    "    # Apply the ReLU activation function element-wise\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def heaviside(x):\n",
    "    # Apply the Heaviside step function element-wise\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def forward_pass(X, W1, W2):\n",
    "    # Compute the input to the hidden layer\n",
    "    X1 = np.dot(X, W1)\n",
    "    \n",
    "    # Apply ReLU activation to the hidden layer\n",
    "    X2 = relu(X1)\n",
    "    \n",
    "    # Compute the output of the network\n",
    "    X3 = np.dot(X2, W2)\n",
    "    \n",
    "    return X1, X2, X3\n",
    "\n",
    "def backward_pass(X, y, W1, W2, X1, X2, X3, learning_rate):\n",
    "    # Compute the error gradient at the output layer\n",
    "    dE_dX3 = -(y - X3) \n",
    "\n",
    "    # Compute the gradient of the loss with respect to the output layer weights\n",
    "    dE_dW2 = np.dot(X2.T, dE_dX3)\n",
    "\n",
    "    # Propagate the error gradient back to the hidden layer\n",
    "    dE_dX2 = np.dot(dE_dX3, W2.T)\n",
    "\n",
    "    # Compute the error gradient at the input of the hidden layer\n",
    "    dE_dX1 = heaviside(X2) * dE_dX2\n",
    "    \n",
    "    # Compute the gradient of the loss with respect to the hidden layer weights\n",
    "    dE_dW1 = np.dot(X.T, dE_dX1)\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    W1 -= learning_rate * dE_dW1\n",
    "    W2 -= learning_rate * dE_dW2\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Define gravitational acceleration\n",
    "g = 9.80665\n",
    "\n",
    "# Compute new variables based on the given formulas\n",
    "g2h2 = g**2 * training_data['height']**2\n",
    "g2hl = g**2 * training_data['height'] * training_data['wave_length']\n",
    "c4 = training_data['speed']**4\n",
    "\n",
    "# Create a new DataFrame with the dimensionally homogeneous inputs and output\n",
    "data = pd.DataFrame({'g2h2': g2h2, 'g2hl': g2hl, 'c4': c4})\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "# Extract the input features and target variable for training\n",
    "X_train = train_data[['g2h2', 'g2hl']].values\n",
    "y_train = train_data['c4'].values.reshape(-1, 1)\n",
    "\n",
    "# Extract the input features and target variable for testing\n",
    "X_test = test_data[['g2h2', 'g2hl']].values\n",
    "y_test = test_data['c4'].values.reshape(-1, 1)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(X_train)):\n",
    "        # Get a single training sample\n",
    "        X_sample = X_train[i].reshape(1, -1)\n",
    "        y_sample = y_train[i].reshape(1, -1)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        X1, X2, X3 = forward_pass(X_sample, W1, W2)\n",
    "        \n",
    "        # Perform backward pass and update weights\n",
    "        W1, W2 = backward_pass(X_sample, y_sample, W1, W2, X1, X2, X3, learning_rate)\n",
    "    \n",
    "    # Print the mean squared error for training data every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        y_pred_train = forward_pass(X_train, W1, W2)[-1]\n",
    "        mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train MSE: {mse_train:.4f}\")\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred_test = forward_pass(X_test, W1, W2)[-1]\n",
    "mse_test = np.mean((y_test - y_pred_test)**2)\n",
    "print(f\"Test Mean Squared Error: {mse_test:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working but not according to addendum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1)\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2)\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dLoss_dZ2 = self.z2 - y\n",
    "        dLoss_dW2 = np.dot(self.a1.T, dLoss_dZ2)\n",
    "        dLoss_dA1 = np.dot(dLoss_dZ2, self.W2.T)\n",
    "        dLoss_dZ1 = dLoss_dA1 * (self.a1 > 0)\n",
    "        dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "\n",
    "        self.W2 -= learning_rate * dLoss_dW2 \n",
    "        self.W1 -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = np.mean((output - y) ** 2)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Extract input features (height and wave_length) and target variable (speed)\n",
    "X = data[['height', 'wave_length']].values\n",
    "y = data['speed'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.001\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "loss_history = network.train(X, y, num_epochs, learning_rate)\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $c^4 = S(g^2h^2, g^2h\\lambda)$ ---------- $(m^4/s^4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = {\n",
    "            'W1': np.random.uniform(-1, 1, size=(input_size, hidden_size)),\n",
    "            'W2': np.random.uniform(-1, 1, size=(hidden_size, output_size))\n",
    "        }\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.weights['W1'])\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights['W2'])\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dLoss_dZ2 = np.dot(2, self.z2) - y\n",
    "        dLoss_dW2 = np.dot(self.a1.T, dLoss_dZ2)\n",
    "        dLoss_dA1 = np.dot(dLoss_dZ2, self.weights['W2'].T)\n",
    "        dLoss_dZ1 = dLoss_dA1 * (self.a1 > 0)\n",
    "        dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "\n",
    "        self.weights['W2'] -= learning_rate * dLoss_dW2 \n",
    "        self.weights['W1'] -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the training data indices\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "\n",
    "            # Perform mini-batch gradient descent\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                # Get the batch indices\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                \n",
    "                # Select the batch data using the shuffled indices\n",
    "                batch_X = X_train[batch_indices]\n",
    "                batch_y = y_train[batch_indices]\n",
    "\n",
    "                # Forward pass and loss calculation for the batch\n",
    "                batch_output = self.forward(batch_X)\n",
    "                batch_loss = np.mean(((batch_output - batch_y)/batch_y) ** 2)\n",
    "                train_loss_history.append(batch_loss)\n",
    "\n",
    "                #print(f'np.max(train_loss_history) = {np.max(train_loss_history)}')\n",
    "\n",
    "                self.backward(batch_X, batch_y, learning_rate)\n",
    "\n",
    "            # Forward pass and loss calculation for validation data\n",
    "            val_output = self.forward(X_val)\n",
    "            val_loss = np.mean((val_output - y_val) ** 2)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {batch_loss:.10f}, Val Loss: {val_loss:.10f}\")\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Load and prepare data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "g = 9.80665  # Gravitational acceleration\n",
    "\n",
    "# Compute dimensionally homogeneous and normalized inputs/output\n",
    "c4 = training_data['speed']**4\n",
    "training_data['g2h2_c4'] = (g**2 * training_data['height']**2) / c4\n",
    "training_data['g2hl_c4'] = (g**2 * training_data['height'] * training_data['wave_length']) / c4\n",
    "X = training_data[[\"g2h2_c4\", \"g2hl_c4\"]].values\n",
    "y = (c4 / c4).values.reshape(-1, 1)  # Normalized target (always 1)\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 100000\n",
    "learning_rate = 0.00000000000001\n",
    "#learning_rate = 0.000000000000000001\n",
    "#learning_rate = 10e-6\n",
    "batch_size = 64\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "train_loss_history, val_loss_history = network.train(X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 45384.8641\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $c^2=S(gh, g \\sqrt{h\\lambda})$ ---------- $(m^2/s^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = {\n",
    "            'W1': np.random.uniform(-1, 1, size=(input_size, hidden_size)),\n",
    "            'W2': np.random.uniform(-1, 1, size=(hidden_size, output_size))\n",
    "        }\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.weights['W1'])\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights['W2'])\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dLoss_dZ2 = np.dot(2, self.z2) - y\n",
    "        dLoss_dW2 = np.dot(self.a1.T, dLoss_dZ2)\n",
    "        dLoss_dA1 = np.dot(dLoss_dZ2, self.weights['W2'].T)\n",
    "        dLoss_dZ1 = dLoss_dA1 * (self.a1 > 0)\n",
    "        dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "\n",
    "        self.weights['W2'] -= learning_rate * dLoss_dW2 \n",
    "        self.weights['W1'] -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the training data indices\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "\n",
    "            # Perform mini-batch gradient descent\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                # Get the batch indices\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                \n",
    "                # Select the batch data using the shuffled indices\n",
    "                batch_X = X_train[batch_indices]\n",
    "                batch_y = y_train[batch_indices]\n",
    "\n",
    "                # Forward pass and loss calculation for the batch\n",
    "                batch_output = self.forward(batch_X)\n",
    "                batch_loss = np.mean(((batch_output - batch_y)/batch_y) ** 2)\n",
    "                train_loss_history.append(batch_loss)\n",
    "\n",
    "                #print(f'np.max(train_loss_history) = {np.max(train_loss_history)}')\n",
    "\n",
    "                self.backward(batch_X, batch_y, learning_rate)\n",
    "\n",
    "            # Forward pass and loss calculation for validation data\n",
    "            val_output = self.forward(X_val)\n",
    "            val_loss = np.mean((val_output - y_val) ** 2)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {batch_loss:.10f}, Val Loss: {val_loss:.10f}\")\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Load and prepare data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "g = 9.80665  # Gravitational acceleration\n",
    "\n",
    "# Compute dimensionally homogeneous and normalized inputs/output\n",
    "c2 = data['speed']**2\n",
    "data['gh_c2'] = (g * data['height']) / c2\n",
    "data['gsl_c2'] = (g * np.sqrt(data['height'] * data['wave_length'])) / c2\n",
    "X = data[[\"gh_c2\", \"gsl_c2\"]].values\n",
    "y = (c2 / c2).values.reshape(-1, 1)  # Normalized target (always 1)\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 100000\n",
    "learning_rate = 0.000000001\n",
    "#learning_rate = 0.000000000000000001\n",
    "#learning_rate = 10e-6\n",
    "batch_size = 64\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "train_loss_history, val_loss_history = network.train(X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 45384.8641\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $c^2 = S(g\\sqrt{h^2+\\lambda^2}, \\frac{gh\\lambda}{h+\\lambda})$ ------------ $(m^2/s^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = {\n",
    "            'W1': np.random.uniform(-1, 1, size=(input_size, hidden_size)),\n",
    "            'W2': np.random.uniform(-1, 1, size=(hidden_size, output_size))\n",
    "        }\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.weights['W1'])\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights['W2'])\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dLoss_dZ2 = np.dot(2, self.z2) - y\n",
    "        dLoss_dW2 = np.dot(self.a1.T, dLoss_dZ2)\n",
    "        dLoss_dA1 = np.dot(dLoss_dZ2, self.weights['W2'].T)\n",
    "        dLoss_dZ1 = dLoss_dA1 * (self.a1 > 0)\n",
    "        dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "\n",
    "        self.weights['W2'] -= learning_rate * dLoss_dW2 \n",
    "        self.weights['W1'] -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the training data indices\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "\n",
    "            # Perform mini-batch gradient descent\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                # Get the batch indices\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                \n",
    "                # Select the batch data using the shuffled indices\n",
    "                batch_X = X_train[batch_indices]\n",
    "                batch_y = y_train[batch_indices]\n",
    "\n",
    "                # Forward pass and loss calculation for the batch\n",
    "                batch_output = self.forward(batch_X)\n",
    "                batch_loss = np.mean(((batch_output - batch_y)/batch_y) ** 2)\n",
    "                train_loss_history.append(batch_loss)\n",
    "\n",
    "                #print(f'np.max(train_loss_history) = {np.max(train_loss_history)}')\n",
    "\n",
    "                self.backward(batch_X, batch_y, learning_rate)\n",
    "\n",
    "            # Forward pass and loss calculation for validation data\n",
    "            val_output = self.forward(X_val)\n",
    "            val_loss = np.mean((val_output - y_val) ** 2)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {batch_loss:.10f}, Val Loss: {val_loss:.10f}\")\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Load and prepare data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "g = 9.80665  # Gravitational acceleration\n",
    "\n",
    "# Compute dimensionally homogeneous and normalized inputs/output\n",
    "c2 = data['speed']**2\n",
    "data['ghl_c2'] = (g * np.sqrt(data['height']**2 + data['wave_length']**2)) / c2\n",
    "data['gl_c2'] = (g * data['height'] * data['wave_length']) / (data['height'] + data['wave_length']) / c2\n",
    "X = data[[\"ghl_c2\", \"gl_c2\"]].values\n",
    "y = (c2 / c2).values.reshape(-1, 1)  # Normalized target (always 1)\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 100000\n",
    "learning_rate = 0.000000001\n",
    "#learning_rate = 0.000000000000000001\n",
    "#learning_rate = 10e-6\n",
    "batch_size = 64\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "train_loss_history, val_loss_history = network.train(X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 45384.8641\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $c = S(h,\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = {\n",
    "            'W1': np.random.uniform(-1, 1, size=(input_size, hidden_size)),\n",
    "            'W2': np.random.uniform(-1, 1, size=(hidden_size, output_size))\n",
    "        }\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.weights['W1'])\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights['W2'])\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dLoss_dZ2 = np.dot(2, self.z2) - y\n",
    "        dLoss_dW2 = np.dot(self.a1.T, dLoss_dZ2)\n",
    "        dLoss_dA1 = np.dot(dLoss_dZ2, self.weights['W2'].T)\n",
    "        dLoss_dZ1 = dLoss_dA1 * (self.a1 > 0)\n",
    "        dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "\n",
    "        self.weights['W2'] -= learning_rate * dLoss_dW2 \n",
    "        self.weights['W1'] -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the training data indices\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "\n",
    "            # Perform mini-batch gradient descent\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                # Get the batch indices\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                \n",
    "                # Select the batch data using the shuffled indices\n",
    "                batch_X = X_train[batch_indices]\n",
    "                batch_y = y_train[batch_indices]\n",
    "\n",
    "                # Forward pass and loss calculation for the batch\n",
    "                batch_output = self.forward(batch_X)\n",
    "                batch_loss = np.mean(((batch_output - batch_y)/batch_y) ** 2)\n",
    "                train_loss_history.append(batch_loss)\n",
    "\n",
    "                #print(f'np.max(train_loss_history) = {np.max(train_loss_history)}')\n",
    "\n",
    "                self.backward(batch_X, batch_y, learning_rate)\n",
    "\n",
    "            # Forward pass and loss calculation for validation data\n",
    "            val_output = self.forward(X_val)\n",
    "            val_loss = np.mean((val_output - y_val) ** 2)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {batch_loss:.10f}, Val Loss: {val_loss:.10f}\")\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Load and prepare data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "X = training_data[['height', 'wave_length']].values\n",
    "y = training_data.speed.values.reshape(-1, 1) \n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 100000\n",
    "learning_rate = 0.000000000001\n",
    "#learning_rate = 0.000000000000000001\n",
    "#learning_rate = 10e-6\n",
    "batch_size = 64\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "train_loss_history, val_loss_history = network.train(X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 45384.8641\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "source": [
    "# Load and prepare data\n",
    "training_data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "training_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = {\n",
    "            'W1': np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size),\n",
    "            'W2': np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "        }\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = np.dot(X, self.weights['W1'])\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = np.dot(a1, self.weights['W2'])\n",
    "        return z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        z2, a1, z1 = self.forward(X)\n",
    "        dLoss_dZ2 = 2 * (z2 - y) / y\n",
    "        dLoss_dW2 = np.dot(a1.T, dLoss_dZ2)\n",
    "        dLoss_dA1 = np.dot(dLoss_dZ2, self.weights['W2'].T)\n",
    "        dLoss_dZ1 = dLoss_dA1 * (a1 > 0)\n",
    "        dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "        self.weights['W2'] -= learning_rate * dLoss_dW2\n",
    "        self.weights['W1'] -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the training data indices\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "\n",
    "            # Perform mini-batch gradient descent\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                batch_X, batch_y = X_train[batch_indices], y_train[batch_indices]\n",
    "                self.backward(batch_X, batch_y, learning_rate)\n",
    "\n",
    "            # Calculate loss for training and validation data\n",
    "            train_loss = self.calculate_loss(X_train, y_train)\n",
    "            val_loss = self.calculate_loss(X_val, y_val)\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        output = self.forward(X)\n",
    "        loss = np.mean(((output - y) / y) ** 2)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"water_waves_training_data.csv\")\n",
    "\n",
    "# Extract input features (height and wave_length) and target variable (speed)\n",
    "X = data[['height', 'wave_length']].values\n",
    "y = data['speed'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_epochs = 100000\n",
    "learning_rate = 0.0000000000000000001\n",
    "#learning_rate = 0.000000000001\n",
    "#learning_rate = 10e-6\n",
    "batch_size = 32\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "train_loss_history, val_loss_history = network.train(X_train, y_train, X_val, y_val, num_epochs, learning_rate, batch_size)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "# Set y-axis limits\n",
    "#plt.ylim(0, max(max(train_loss_history), max(val_loss_history)) * 1.1)  # 10% buffer\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "train_accuracy = np.mean((network.forward(X_train) > 0.5) == y_train)\n",
    "val_accuracy = np.mean((network.forward(X_val) > 0.5) == y_val)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Training', 'Validation'], [train_accuracy, val_accuracy])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the network on the validation set\n",
    "val_output = network.forward(X_val)\n",
    "print(f'p.mean(val_output-y_val) = {np.mean(val_output-y_val)}')\n",
    "print(f'np.mean(val_output) = {np.mean(val_output)}')\n",
    "print(f'np.mean(y_val) = {np.mean(y_val)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# According to addendum but not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ShallowReLUNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Initialize weights randomly with Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1)\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2)\n",
    "        return self.z2\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dLoss_dZ2 = self.z2 - y\n",
    "        dLoss_dW2 = np.dot(dLoss_dZ2, self.a1)\n",
    "        dLoss_dA1 = np.dot(self.W2, dLoss_dZ2)\n",
    "        dLoss_dZ1 = (self.a1 > 0).T * dLoss_dA1\n",
    "        dLoss_dW1 = np.dot(dLoss_dZ1, X)\n",
    "\n",
    "        self.W2 -= learning_rate * dLoss_dW2 \n",
    "        self.W1 -= learning_rate * dLoss_dW1\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = np.mean((output - y) ** 2)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = ShallowReLUNetwork(input_size, hidden_size, output_size)\n",
    "loss_history = network.train(X, y, num_epochs, learning_rate)\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
